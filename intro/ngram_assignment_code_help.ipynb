{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT_A2wjHn-iD"
      },
      "source": [
        "## Working with the Brown Corpus (using nltk):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4E6cziun-iF"
      },
      "source": [
        "To get the Borwn corpus, do **either** this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0j9pQkhKn-iG",
        "outputId": "ab25f998-6eff-4fc7-fbf6-a7dc355bb2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D1sgAfNn-iH"
      },
      "source": [
        "of if that doesn't work, **this**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OO7aS7v5n-iH"
      },
      "outputs": [],
      "source": [
        "#from nltk.corpus import download\n",
        "#download(\"brown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1LFaLBsn-iH"
      },
      "source": [
        "The following code cell assigns a list of all 1.2 M word tokens in `brown` to the name `bw`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8iwsynuBn-iI"
      },
      "outputs": [],
      "source": [
        "bw = brown.words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sHpY1yxn-iI"
      },
      "source": [
        "The value of the name `bw`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a6AsU-esn-iI",
        "outputId": "635d0988-8881-4718-ddd9-b3edc3fa6536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "bw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTOLrmtn-iJ"
      },
      "source": [
        "To get a data structure with all the word counts, do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-MjJORAwn-iJ"
      },
      "outputs": [],
      "source": [
        "fd = nltk.FreqDist(bw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2l2iCxJn-iJ"
      },
      "source": [
        "What `fd` is is an`nltk FreqDist` (Frequency Distribution), essentially, a Python dictionary containing the count of all the word types in Brown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "crP8yUpIn-iJ",
        "outputId": "e8f6c2f7-634d-4592-d1ec-1956b6b4d2a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "fd[\"computer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5ks8IZwn-iK"
      },
      "source": [
        "That means there are 13 *tokens* of the word type 'computer' in the Brown corpus.  Note that case matters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F7HRPOXln-iK",
        "outputId": "2f5950b3-bd66-4ff3-fe79-18c5d7c48cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "fd[\"Computer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "id": "VZA1Czqun-iK",
        "outputId": "3a0be656-c4b3-48b0-e794-e85e5490cc80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62713, 7258)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "fd['the'],fd['The']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1M-6827n-iK"
      },
      "source": [
        "The `fd` FreqDist also has some features customized for word frequencies, like the `most_common` method), which allows you to find the top n most frequent words and their counts. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XFZZBGUDn-iK",
        "outputId": "35958276-d8ca-435d-9783-775b89dd2b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 62713), (',', 58334), ('.', 49346)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "fd.most_common(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjNZQmTqn-iK"
      },
      "source": [
        "To find the number of word tokens in Brown, find the length of `bw`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LO9cbXNQn-iK",
        "outputId": "fe89ec2d-27f3-4dfd-cb29-71ecb34294e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(bw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLw1zAGn-iK"
      },
      "source": [
        "So, about 1.16 Million words.  You can also do this more efficiently, by using another one of the customized methods provided by a `FreqDist`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EFgVgApKn-iK",
        "outputId": "175778ab-d5d4-46ef-c98c-252be3e5e0e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "fd.N()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RQDdT61n-iL"
      },
      "source": [
        "To find out the number of **word types** in Brown, just get the length of the `fd` dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "En5lY1ogn-iL",
        "outputId": "05df424d-5afa-4530-ae07-d7db0f38052f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56057"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(fd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCdwYQAZn-iL"
      },
      "source": [
        "It is handy to ignore case for purposes of this exercise. To do that, just lowercase all the words in Brown before making the frequency distribution. This changes the number of word types, but not the number of word tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zbK6_gDon-iL",
        "outputId": "4240e241-c21b-4d2c-ca31-3743db501823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# This uses what Pythonistas call a list comprehension.\n",
        "# It's just a compact way of writing a loop and collecting results in  a list.\n",
        "bw = brown.words()\n",
        "bw = [w.lower() for w in bw]\n",
        "len(bw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuLGQOX-n-iL"
      },
      "source": [
        "The length of the **corpus** (the body of data) hasn't changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1BXnM1vSn-iL",
        "outputId": "e766d7ac-e6c7-41e4-9a4c-65f282975fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49815"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "fd = nltk.FreqDist(bw)\n",
        "len(fd)\n",
        "#49815"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL_d6F4_n-iL"
      },
      "source": [
        "The dictionary has shrunk because we now ignore the distinction between 'The' and 'the'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4EZZWwn9n-iL",
        "outputId": "96c8d0a0-abec-4665-a3be-f4b83a461056",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69971"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "fd['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "09inolaGn-iL",
        "outputId": "9edd8314-66d4-4dd8-c046-96801619270a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "fd['The']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-blYWQ85n-iL"
      },
      "source": [
        "You may find it useful to take a look at [Chapter One of the NLTK book.](http://www.nltk.org/book/ch01.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwFSdvXAn-iM"
      },
      "source": [
        "### Constructing a bigram model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cvTdKRB6n-iM"
      },
      "outputs": [],
      "source": [
        "# The Brown Corpus\n",
        "from nltk.corpus import brown\n",
        "from nltk import FreqDist\n",
        "from nltk import bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pPnpvd0n-iM"
      },
      "source": [
        "Here is some helpful code for computing the counts in the **Brown Corpus** needed for a bigram model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wFexzH3cn-iM"
      },
      "outputs": [],
      "source": [
        "#Lowers case all the words\n",
        "words = [w.lower() for w in brown.words()]\n",
        "fd = FreqDist(words)\n",
        "fd2 = FreqDist(bigrams(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBccvUdOn-iM"
      },
      "source": [
        "Number of times the word *bald* appeared in the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pSV5gHjZn-iM"
      },
      "outputs": [],
      "source": [
        "fd[\"bald\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkjPUIEmn-iM"
      },
      "source": [
        "Number of times the bigram *stormy weather* appeared in  the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKKANZEtn-iS"
      },
      "outputs": [],
      "source": [
        "fd2[\"stormy\",\"weather\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWh2qakTn-iT"
      },
      "source": [
        "Total count of the word tokens in the Brown corpus is unchanged:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "g-rFZD8Wn-iT"
      },
      "outputs": [],
      "source": [
        "fd.N()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38fLIXw3n-iT"
      },
      "source": [
        "Total count of all the bigram tokens in the Brown corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wntw_LXSn-iT"
      },
      "outputs": [],
      "source": [
        "fd2.N()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ats6HU4hn-iT"
      },
      "source": [
        "Twenty most common words in the Brown corpus together with their counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g69qrAg0n-iT"
      },
      "outputs": [],
      "source": [
        "fd.most_common(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hph1dGW3n-iT"
      },
      "source": [
        "Twenty most common bigrams in the corpus together with their counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "2dL1BPRNn-iT"
      },
      "outputs": [],
      "source": [
        "fd2.most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EdoHep2n-iT"
      },
      "outputs": [],
      "source": [
        "def restrict_most_common (fd,f_rest):\n",
        "    \"\"\"\n",
        "    Returns the x that satisfy restriction f_rest in sorted order\n",
        "    according to fd.most_common()\n",
        "    \"\"\"\n",
        "    bs,cts = zip(*[(x,ct) for (x,ct) in fd.most_common() if f_rest(x)])\n",
        "    return bs, np.array(cts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAelTHPfn-iT"
      },
      "source": [
        "Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J41S0k6qn-iT"
      },
      "outputs": [],
      "source": [
        "bs,cts = restrict_most_common (fd, lambda  x:x.startswith(\"grou\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og4pVPDCn-iU"
      },
      "outputs": [],
      "source": [
        "bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh2hESA9n-iU"
      },
      "outputs": [],
      "source": [
        "cts"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}